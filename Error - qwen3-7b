Error executing job with overrides: []
Traceback (most recent call last):
  File "/data/jagan/MindSpeed-RL-master/cli/train_grpo.py", line 576, in <module>
    main()
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/data/jagan/MindSpeed-RL-master/cli/train_grpo.py", line 572, in main
    ray.get(train.remote(config))
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/ray/_private/worker.py", line 2772, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/ray/_private/worker.py", line 919, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(RuntimeError): ray::train() (pid=39406, ip=71.72.2.231)
  File "/data/jagan/MindSpeed-RL-master/cli/train_grpo.py", line 160, in train
    actor_worker.wait_all_ref_objs_run_over()
  File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/scheduler/launcher.py", line 268, in wait_all_ref_objs_run_over
    ray.get(self.temp_actor_ref_objs)
ray.exceptions.RayTaskError(RuntimeError): ray::IntegratedWorker.initialize() (pid=39740, ip=71.72.2.231, actor_id=ec415ec6c1d5003ccb6367642b000000, repr=<mindspeed_rl.workers.integrated_worker.IntegratedWorker object at 0xffd0a72cd720>)
  File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/integrated_worker.py", line 84, in initialize
    ActorHybridWorkerBase.initialize(self)
  File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 88, in initialize
    self.model, self.optimizer, self.opt_param_scheduler = self._build_model_optimizer()
  File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 445, in _build_model_optimizer
    actor_module, optimizer, opt_param_scheduler = self.setup_model_and_optimizer(
  File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 552, in setup_model_and_optimizer
    timers('load-checkpoint', log_level=0).start(barrier=True)
  File "/data/jagan/MindSpeed-RL-master/megatron/core/timers.py", line 92, in start
    torch.distributed.barrier(group=self._barrier_group)
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 83, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 4159, in barrier
    work = group.barrier(opts=opts)
RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:102 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
[ERROR] 2025-07-09-14:41:23 (PID:39740, Device:0, RankID:0) ERR02200 DIST call hccl api failed.
/usr/local/python3.10.16/lib/python3.10/tempfile.py:869: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpkai2a9ug'>
  _warnings.warn(warn_message, ResourceWarning)
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=39740) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(train pid=39406) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::IntegratedWorker.initialize() (pid=40158, ip=71.72.2.231, actor_id=a8c1acc606fc0cc52b5865f32b000000, repr=<mindspeed_rl.workers.integrated_worker.IntegratedWorker object at 0xffd08fee1540>)
(train pid=39406)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/integrated_worker.py", line 84, in initialize
(train pid=39406)     ActorHybridWorkerBase.initialize(self)
(train pid=39406)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 88, in initialize
(train pid=39406)     self.model, self.optimizer, self.opt_param_scheduler = self._build_model_optimizer()
(train pid=39406)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 445, in _build_model_optimizer
(train pid=39406)     actor_module, optimizer, opt_param_scheduler = self.setup_model_and_optimizer(
(train pid=39406)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 552, in setup_model_and_optimizer
(train pid=39406)     timers('load-checkpoint', log_level=0).start(barrier=True)
(train pid=39406)   File "/data/jagan/MindSpeed-RL-master/megatron/core/timers.py", line 92, in start
(train pid=39406)     torch.distributed.barrier(group=self._barrier_group)
(train pid=39406)   File "/usr/local/python3.10.16/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 83, in wrapper
(train pid=39406)     return func(*args, **kwargs)
(train pid=39406)   File "/usr/local/python3.10.16/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 4159, in barrier
(train pid=39406)     work = group.barrier(opts=opts)
(train pid=39406) RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:102 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
(train pid=39406) [ERROR] 2025-07-09-14:41:24 (PID:40158, Device:0, RankID:8) ERR02200 DIST call hccl api failed.
