Error executing job with overrides: []
Traceback (most recent call last):
  File "/data/jagan/MindSpeed-RL-master/cli/train_grpo.py", line 576, in <module>
    main()
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/data/jagan/MindSpeed-RL-master/cli/train_grpo.py", line 572, in main
    ray.get(train.remote(config))
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/ray/_private/worker.py", line 2772, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/ray/_private/worker.py", line 919, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(RuntimeError): ray::train() (pid=23066, ip=71.72.2.231)
  File "/data/jagan/MindSpeed-RL-master/cli/train_grpo.py", line 160, in train
    actor_worker.wait_all_ref_objs_run_over()
  File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/scheduler/launcher.py", line 268, in wait_all_ref_objs_run_over
    ray.get(self.temp_actor_ref_objs)
ray.exceptions.RayTaskError(RuntimeError): ray::IntegratedWorker.initialize() (pid=23605, ip=71.72.2.231, actor_id=4264fbae24d45c39a3c82e1329000000, repr=<mindspeed_rl.workers.integrated_worker.IntegratedWorker object at 0xffd07d94d840>)
  File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/integrated_worker.py", line 84, in initialize
    ActorHybridWorkerBase.initialize(self)
  File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 88, in initialize
    self.model, self.optimizer, self.opt_param_scheduler = self._build_model_optimizer()
  File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 445, in _build_model_optimizer
    actor_module, optimizer, opt_param_scheduler = self.setup_model_and_optimizer(
  File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 535, in setup_model_and_optimizer
    model = get_model(model_provider_func, model_type)
  File "/data/jagan/MindSpeed-RL-master/mindspeed_llm/training/training.py", line 149, in wrapper
    model = fn(model_provider_func, model_type, wrap_with_ddp)
  File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in get_model
    model = [DDP(config,
  File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in <listcomp>
    model = [DDP(config,
  File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 419, in distributed_data_parallel_init_with_cp
    self.buffers = allocate_buffers_for_parameters(
  File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 386, in allocate_buffers_for_parameters
    ParamAndGradBuffer(
  File "/data/jagan/MindSpeed-RL-master/megatron/core/distributed/param_and_grad_buffer.py", line 366, in __init__
    self.grad_data = torch.zeros(
  File "/usr/local/python3.10.16/lib/python3.10/site-packages/torch_npu/contrib/transfer_to_npu.py", line 153, in decorated
    return fn(*args, **kwargs)
RuntimeError: NPU out of memory. Tried to allocate 7.09 GiB (NPU 1; 60.96 GiB total capacity; 7.09 GiB already allocated; 7.09 GiB current active; 69.22 MiB free; 7.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.
(train pid=23066) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::IntegratedWorker.initialize() (pid=23745, ip=71.72.2.231, actor_id=c84c93bc3c6bff3dac197ed529000000, repr=<mindspeed_rl.workers.integrated_worker.IntegratedWorker object at 0xffd0a64815d0>)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/integrated_worker.py", line 84, in initialize
(train pid=23066)     ActorHybridWorkerBase.initialize(self)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 88, in initialize
(train pid=23066)     self.model, self.optimizer, self.opt_param_scheduler = self._build_model_optimizer()
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 445, in _build_model_optimizer
(train pid=23066)     actor_module, optimizer, opt_param_scheduler = self.setup_model_and_optimizer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 535, in setup_model_and_optimizer
(train pid=23066)     model = get_model(model_provider_func, model_type)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_llm/training/training.py", line 149, in wrapper
(train pid=23066)     model = fn(model_provider_func, model_type, wrap_with_ddp)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in get_model
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in <listcomp>
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 419, in distributed_data_parallel_init_with_cp
(train pid=23066)     self.buffers = allocate_buffers_for_parameters(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 386, in allocate_buffers_for_parameters
(train pid=23066)     ParamAndGradBuffer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/core/distributed/param_and_grad_buffer.py", line 366, in __init__
(train pid=23066)     self.grad_data = torch.zeros(
(train pid=23066)   File "/usr/local/python3.10.16/lib/python3.10/site-packages/torch_npu/contrib/transfer_to_npu.py", line 153, in decorated
(train pid=23066)     return fn(*args, **kwargs)
(train pid=23066) RuntimeError: NPU out of memory. Tried to allocate 7.09 GiB (NPU 3; 60.96 GiB total capacity; 7.09 GiB already allocated; 7.09 GiB current active; 67.70 MiB free; 7.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.
/usr/local/python3.10.16/lib/python3.10/tempfile.py:869: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp7wmovhwc'>
  _warnings.warn(warn_message, ResourceWarning)
(train pid=23066) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::IntegratedWorker.initialize() (pid=23745, ip=71.72.2.231, actor_id=c84c93bc3c6bff3dac197ed529000000, repr=<mindspeed_rl.workers.integrated_worker.IntegratedWorker object at 0xffd0a64815d0>)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/integrated_worker.py", line 84, in initialize
(train pid=23066)     ActorHybridWorkerBase.initialize(self)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 88, in initialize
(train pid=23066)     self.model, self.optimizer, self.opt_param_scheduler = self._build_model_optimizer()
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 445, in _build_model_optimizer
(train pid=23066)     actor_module, optimizer, opt_param_scheduler = self.setup_model_and_optimizer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 535, in setup_model_and_optimizer
(train pid=23066)     model = get_model(model_provider_func, model_type)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_llm/training/training.py", line 149, in wrapper
(train pid=23066)     model = fn(model_provider_func, model_type, wrap_with_ddp)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in get_model
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in <listcomp>
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 419, in distributed_data_parallel_init_with_cp
(train pid=23066)     self.buffers = allocate_buffers_for_parameters(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 386, in allocate_buffers_for_parameters
(train pid=23066)     ParamAndGradBuffer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/core/distributed/param_and_grad_buffer.py", line 366, in __init__
(train pid=23066)     self.grad_data = torch.zeros(
(train pid=23066)   File "/usr/local/python3.10.16/lib/python3.10/site-packages/torch_npu/contrib/transfer_to_npu.py", line 153, in decorated
(train pid=23066)     return fn(*args, **kwargs)
(train pid=23066) RuntimeError: NPU out of memory. Tried to allocate 7.09 GiB (NPU 3; 60.96 GiB total capacity; 7.09 GiB already allocated; 7.09 GiB current active; 67.70 MiB free; 7.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.
(train pid=23066) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::IntegratedWorker.initialize() (pid=23745, ip=71.72.2.231, actor_id=c84c93bc3c6bff3dac197ed529000000, repr=<mindspeed_rl.workers.integrated_worker.IntegratedWorker object at 0xffd0a64815d0>)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/integrated_worker.py", line 84, in initialize
(train pid=23066)     ActorHybridWorkerBase.initialize(self)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 88, in initialize
(train pid=23066)     self.model, self.optimizer, self.opt_param_scheduler = self._build_model_optimizer()
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 445, in _build_model_optimizer
(train pid=23066)     actor_module, optimizer, opt_param_scheduler = self.setup_model_and_optimizer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 535, in setup_model_and_optimizer
(train pid=23066)     model = get_model(model_provider_func, model_type)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_llm/training/training.py", line 149, in wrapper
(train pid=23066)     model = fn(model_provider_func, model_type, wrap_with_ddp)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in get_model
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in <listcomp>
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 419, in distributed_data_parallel_init_with_cp
(train pid=23066)     self.buffers = allocate_buffers_for_parameters(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 386, in allocate_buffers_for_parameters
(train pid=23066)     ParamAndGradBuffer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/core/distributed/param_and_grad_buffer.py", line 366, in __init__
(train pid=23066)     self.grad_data = torch.zeros(
(train pid=23066)   File "/usr/local/python3.10.16/lib/python3.10/site-packages/torch_npu/contrib/transfer_to_npu.py", line 153, in decorated
(train pid=23066)     return fn(*args, **kwargs)
(train pid=23066) RuntimeError: NPU out of memory. Tried to allocate 7.09 GiB (NPU 3; 60.96 GiB total capacity; 7.09 GiB already allocated; 7.09 GiB current active; 67.70 MiB free; 7.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.
(train pid=23066) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::IntegratedWorker.initialize() (pid=23759, ip=71.72.2.231, actor_id=c5177efe70196af28639821d29000000, repr=<mindspeed_rl.workers.integrated_worker.IntegratedWorker object at 0xffd0860ad690>)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/integrated_worker.py", line 84, in initialize
(train pid=23066)     ActorHybridWorkerBase.initialize(self)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 88, in initialize
(train pid=23066)     self.model, self.optimizer, self.opt_param_scheduler = self._build_model_optimizer()
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 445, in _build_model_optimizer
(train pid=23066)     actor_module, optimizer, opt_param_scheduler = self.setup_model_and_optimizer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 535, in setup_model_and_optimizer
(train pid=23066)     model = get_model(model_provider_func, model_type)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_llm/training/training.py", line 149, in wrapper
(train pid=23066)     model = fn(model_provider_func, model_type, wrap_with_ddp)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in get_model
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in <listcomp>
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 419, in distributed_data_parallel_init_with_cp
(train pid=23066)     self.buffers = allocate_buffers_for_parameters(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 386, in allocate_buffers_for_parameters
(train pid=23066)     ParamAndGradBuffer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/core/distributed/param_and_grad_buffer.py", line 366, in __init__
(train pid=23066)     self.grad_data = torch.zeros(
(train pid=23066)   File "/usr/local/python3.10.16/lib/python3.10/site-packages/torch_npu/contrib/transfer_to_npu.py", line 153, in decorated
(train pid=23066)     return fn(*args, **kwargs)
(train pid=23066) RuntimeError: NPU out of memory. Tried to allocate 7.09 GiB (NPU 7; 60.96 GiB total capacity; 7.09 GiB already allocated; 7.09 GiB current active; 213.65 MiB free; 7.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.
(train pid=23066) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::IntegratedWorker.initialize() (pid=23750, ip=71.72.2.231, actor_id=5ef95231409e43a313feb4b129000000, repr=<mindspeed_rl.workers.integrated_worker.IntegratedWorker object at 0xffd090111600>)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/integrated_worker.py", line 84, in initialize
(train pid=23066)     ActorHybridWorkerBase.initialize(self)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 88, in initialize
(train pid=23066)     self.model, self.optimizer, self.opt_param_scheduler = self._build_model_optimizer()
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 445, in _build_model_optimizer
(train pid=23066)     actor_module, optimizer, opt_param_scheduler = self.setup_model_and_optimizer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 535, in setup_model_and_optimizer
(train pid=23066)     model = get_model(model_provider_func, model_type)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_llm/training/training.py", line 149, in wrapper
(train pid=23066)     model = fn(model_provider_func, model_type, wrap_with_ddp)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in get_model
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in <listcomp>
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 419, in distributed_data_parallel_init_with_cp
(train pid=23066)     self.buffers = allocate_buffers_for_parameters(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 386, in allocate_buffers_for_parameters
(train pid=23066)     ParamAndGradBuffer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/core/distributed/param_and_grad_buffer.py", line 366, in __init__
(train pid=23066)     self.grad_data = torch.zeros(
(train pid=23066)   File "/usr/local/python3.10.16/lib/python3.10/site-packages/torch_npu/contrib/transfer_to_npu.py", line 153, in decorated
(train pid=23066)     return fn(*args, **kwargs)
(train pid=23066) RuntimeError: NPU out of memory. Tried to allocate 7.09 GiB (NPU 5; 60.96 GiB total capacity; 7.09 GiB already allocated; 7.09 GiB current active; 213.60 MiB free; 7.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.
(train pid=23066) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::IntegratedWorker.initialize() (pid=23745, ip=71.72.2.231, actor_id=c84c93bc3c6bff3dac197ed529000000, repr=<mindspeed_rl.workers.integrated_worker.IntegratedWorker object at 0xffd0a64815d0>)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/integrated_worker.py", line 84, in initialize
(train pid=23066)     ActorHybridWorkerBase.initialize(self)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 88, in initialize
(train pid=23066)     self.model, self.optimizer, self.opt_param_scheduler = self._build_model_optimizer()
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 445, in _build_model_optimizer
(train pid=23066)     actor_module, optimizer, opt_param_scheduler = self.setup_model_and_optimizer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 535, in setup_model_and_optimizer
(train pid=23066)     model = get_model(model_provider_func, model_type)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_llm/training/training.py", line 149, in wrapper
(train pid=23066)     model = fn(model_provider_func, model_type, wrap_with_ddp)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in get_model
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in <listcomp>
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 419, in distributed_data_parallel_init_with_cp
(train pid=23066)     self.buffers = allocate_buffers_for_parameters(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 386, in allocate_buffers_for_parameters
(train pid=23066)     ParamAndGradBuffer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/core/distributed/param_and_grad_buffer.py", line 366, in __init__
(train pid=23066)     self.grad_data = torch.zeros(
(train pid=23066)   File "/usr/local/python3.10.16/lib/python3.10/site-packages/torch_npu/contrib/transfer_to_npu.py", line 153, in decorated
(train pid=23066)     return fn(*args, **kwargs)
(train pid=23066) RuntimeError: NPU out of memory. Tried to allocate 7.09 GiB (NPU 3; 60.96 GiB total capacity; 7.09 GiB already allocated; 7.09 GiB current active; 67.70 MiB free; 7.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.
(train pid=23066) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::IntegratedWorker.initialize() (pid=23759, ip=71.72.2.231, actor_id=c5177efe70196af28639821d29000000, repr=<mindspeed_rl.workers.integrated_worker.IntegratedWorker object at 0xffd0860ad690>)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/integrated_worker.py", line 84, in initialize
(train pid=23066)     ActorHybridWorkerBase.initialize(self)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 88, in initialize
(train pid=23066)     self.model, self.optimizer, self.opt_param_scheduler = self._build_model_optimizer()
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 445, in _build_model_optimizer
(train pid=23066)     actor_module, optimizer, opt_param_scheduler = self.setup_model_and_optimizer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 535, in setup_model_and_optimizer
(train pid=23066)     model = get_model(model_provider_func, model_type)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_llm/training/training.py", line 149, in wrapper
(train pid=23066)     model = fn(model_provider_func, model_type, wrap_with_ddp)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in get_model
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in <listcomp>
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 419, in distributed_data_parallel_init_with_cp
(train pid=23066)     self.buffers = allocate_buffers_for_parameters(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 386, in allocate_buffers_for_parameters
(train pid=23066)     ParamAndGradBuffer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/core/distributed/param_and_grad_buffer.py", line 366, in __init__
(train pid=23066)     self.grad_data = torch.zeros(
(train pid=23066)   File "/usr/local/python3.10.16/lib/python3.10/site-packages/torch_npu/contrib/transfer_to_npu.py", line 153, in decorated
(train pid=23066)     return fn(*args, **kwargs)
(train pid=23066) RuntimeError: NPU out of memory. Tried to allocate 7.09 GiB (NPU 7; 60.96 GiB total capacity; 7.09 GiB already allocated; 7.09 GiB current active; 213.65 MiB free; 7.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.
(train pid=23066) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::IntegratedWorker.initialize() (pid=23750, ip=71.72.2.231, actor_id=5ef95231409e43a313feb4b129000000, repr=<mindspeed_rl.workers.integrated_worker.IntegratedWorker object at 0xffd090111600>)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/integrated_worker.py", line 84, in initialize
(train pid=23066)     ActorHybridWorkerBase.initialize(self)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 88, in initialize
(train pid=23066)     self.model, self.optimizer, self.opt_param_scheduler = self._build_model_optimizer()
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 445, in _build_model_optimizer
(train pid=23066)     actor_module, optimizer, opt_param_scheduler = self.setup_model_and_optimizer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 535, in setup_model_and_optimizer
(train pid=23066)     model = get_model(model_provider_func, model_type)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_llm/training/training.py", line 149, in wrapper
(train pid=23066)     model = fn(model_provider_func, model_type, wrap_with_ddp)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in get_model
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in <listcomp>
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 419, in distributed_data_parallel_init_with_cp
(train pid=23066)     self.buffers = allocate_buffers_for_parameters(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 386, in allocate_buffers_for_parameters
(train pid=23066)     ParamAndGradBuffer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/core/distributed/param_and_grad_buffer.py", line 366, in __init__
(train pid=23066)     self.grad_data = torch.zeros(
(train pid=23066)   File "/usr/local/python3.10.16/lib/python3.10/site-packages/torch_npu/contrib/transfer_to_npu.py", line 153, in decorated
(train pid=23066)     return fn(*args, **kwargs)
(train pid=23066) RuntimeError: NPU out of memory. Tried to allocate 7.09 GiB (NPU 5; 60.96 GiB total capacity; 7.09 GiB already allocated; 7.09 GiB current active; 213.60 MiB free; 7.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.
(train pid=23066) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::IntegratedWorker.initialize() (pid=23759, ip=71.72.2.231, actor_id=c5177efe70196af28639821d29000000, repr=<mindspeed_rl.workers.integrated_worker.IntegratedWorker object at 0xffd0860ad690>)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/integrated_worker.py", line 84, in initialize
(train pid=23066)     ActorHybridWorkerBase.initialize(self)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 88, in initialize
(train pid=23066)     self.model, self.optimizer, self.opt_param_scheduler = self._build_model_optimizer()
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 445, in _build_model_optimizer
(train pid=23066)     actor_module, optimizer, opt_param_scheduler = self.setup_model_and_optimizer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 535, in setup_model_and_optimizer
(train pid=23066)     model = get_model(model_provider_func, model_type)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_llm/training/training.py", line 149, in wrapper
(train pid=23066)     model = fn(model_provider_func, model_type, wrap_with_ddp)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in get_model
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in <listcomp>
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 419, in distributed_data_parallel_init_with_cp
(train pid=23066)     self.buffers = allocate_buffers_for_parameters(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 386, in allocate_buffers_for_parameters
(train pid=23066)     ParamAndGradBuffer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/core/distributed/param_and_grad_buffer.py", line 366, in __init__
(train pid=23066)     self.grad_data = torch.zeros(
(train pid=23066)   File "/usr/local/python3.10.16/lib/python3.10/site-packages/torch_npu/contrib/transfer_to_npu.py", line 153, in decorated
(train pid=23066)     return fn(*args, **kwargs)
(train pid=23066) RuntimeError: NPU out of memory. Tried to allocate 7.09 GiB (NPU 7; 60.96 GiB total capacity; 7.09 GiB already allocated; 7.09 GiB current active; 213.65 MiB free; 7.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.
(train pid=23066) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::IntegratedWorker.initialize() (pid=23750, ip=71.72.2.231, actor_id=5ef95231409e43a313feb4b129000000, repr=<mindspeed_rl.workers.integrated_worker.IntegratedWorker object at 0xffd090111600>)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/integrated_worker.py", line 84, in initialize
(train pid=23066)     ActorHybridWorkerBase.initialize(self)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 88, in initialize
(train pid=23066)     self.model, self.optimizer, self.opt_param_scheduler = self._build_model_optimizer()
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 445, in _build_model_optimizer
(train pid=23066)     actor_module, optimizer, opt_param_scheduler = self.setup_model_and_optimizer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 535, in setup_model_and_optimizer
(train pid=23066)     model = get_model(model_provider_func, model_type)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_llm/training/training.py", line 149, in wrapper
(train pid=23066)     model = fn(model_provider_func, model_type, wrap_with_ddp)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in get_model
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in <listcomp>
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 419, in distributed_data_parallel_init_with_cp
(train pid=23066)     self.buffers = allocate_buffers_for_parameters(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 386, in allocate_buffers_for_parameters
(train pid=23066)     ParamAndGradBuffer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/core/distributed/param_and_grad_buffer.py", line 366, in __init__
(train pid=23066)     self.grad_data = torch.zeros(
(train pid=23066)   File "/usr/local/python3.10.16/lib/python3.10/site-packages/torch_npu/contrib/transfer_to_npu.py", line 153, in decorated
(train pid=23066)     return fn(*args, **kwargs)
(train pid=23066) RuntimeError: NPU out of memory. Tried to allocate 7.09 GiB (NPU 5; 60.96 GiB total capacity; 7.09 GiB already allocated; 7.09 GiB current active; 213.60 MiB free; 7.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.
(IntegratedWorker pid=23605) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=23605) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=23605) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=23605) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=23605) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=23605) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=23605) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(IntegratedWorker pid=23605) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
(train pid=23066) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::IntegratedWorker.initialize() (pid=23759, ip=71.72.2.231, actor_id=c5177efe70196af28639821d29000000, repr=<mindspeed_rl.workers.integrated_worker.IntegratedWorker object at 0xffd0860ad690>)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/integrated_worker.py", line 84, in initialize
(train pid=23066)     ActorHybridWorkerBase.initialize(self)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 88, in initialize
(train pid=23066)     self.model, self.optimizer, self.opt_param_scheduler = self._build_model_optimizer()
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 445, in _build_model_optimizer
(train pid=23066)     actor_module, optimizer, opt_param_scheduler = self.setup_model_and_optimizer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 535, in setup_model_and_optimizer
(train pid=23066)     model = get_model(model_provider_func, model_type)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_llm/training/training.py", line 149, in wrapper
(train pid=23066)     model = fn(model_provider_func, model_type, wrap_with_ddp)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in get_model
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in <listcomp>
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 419, in distributed_data_parallel_init_with_cp
(train pid=23066)     self.buffers = allocate_buffers_for_parameters(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 386, in allocate_buffers_for_parameters
(train pid=23066)     ParamAndGradBuffer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/core/distributed/param_and_grad_buffer.py", line 366, in __init__
(train pid=23066)     self.grad_data = torch.zeros(
(train pid=23066)   File "/usr/local/python3.10.16/lib/python3.10/site-packages/torch_npu/contrib/transfer_to_npu.py", line 153, in decorated
(train pid=23066)     return fn(*args, **kwargs)
(train pid=23066) RuntimeError: NPU out of memory. Tried to allocate 7.09 GiB (NPU 7; 60.96 GiB total capacity; 7.09 GiB already allocated; 7.09 GiB current active; 213.65 MiB free; 7.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.
(train pid=23066) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::IntegratedWorker.initialize() (pid=23750, ip=71.72.2.231, actor_id=5ef95231409e43a313feb4b129000000, repr=<mindspeed_rl.workers.integrated_worker.IntegratedWorker object at 0xffd090111600>)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/integrated_worker.py", line 84, in initialize
(train pid=23066)     ActorHybridWorkerBase.initialize(self)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 88, in initialize
(train pid=23066)     self.model, self.optimizer, self.opt_param_scheduler = self._build_model_optimizer()
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_rl/workers/actor_hybrid_worker.py", line 445, in _build_model_optimizer
(train pid=23066)     actor_module, optimizer, opt_param_scheduler = self.setup_model_and_optimizer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 535, in setup_model_and_optimizer
(train pid=23066)     model = get_model(model_provider_func, model_type)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed_llm/training/training.py", line 149, in wrapper
(train pid=23066)     model = fn(model_provider_func, model_type, wrap_with_ddp)
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in get_model
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/training/training.py", line 453, in <listcomp>
(train pid=23066)     model = [DDP(config,
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 419, in distributed_data_parallel_init_with_cp
(train pid=23066)     self.buffers = allocate_buffers_for_parameters(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/mindspeed/core/data_parallel/distributed_data_parallel.py", line 386, in allocate_buffers_for_parameters
(train pid=23066)     ParamAndGradBuffer(
(train pid=23066)   File "/data/jagan/MindSpeed-RL-master/megatron/core/distributed/param_and_grad_buffer.py", line 366, in __init__
(train pid=23066)     self.grad_data = torch.zeros(
(train pid=23066)   File "/usr/local/python3.10.16/lib/python3.10/site-packages/torch_npu/contrib/transfer_to_npu.py", line 153, in decorated
(train pid=23066)     return fn(*args, **kwargs)
(train pid=23066) RuntimeError: NPU out of memory. Tried to allocate 7.09 GiB (NPU 5; 60.96 GiB total capacity; 7.09 GiB already allocated; 7.09 GiB current active; 213.60 MiB free; 7.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.
(IntegratedWorker pid=23605) Process ForkServerProcess-10:
(IntegratedWorker pid=23605) Traceback (most recent call last):
(IntegratedWorker pid=23605)   File "/usr/local/python3.10.16/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
(IntegratedWorker pid=23605)     self.run()
(IntegratedWorker pid=23605)   File "/usr/local/python3.10.16/lib/python3.10/multiprocessing/process.py", line 108, in run
(IntegratedWorker pid=23605)     self._target(*self._args, **self._kwargs)
(IntegratedWorker pid=23605)   File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/utils/common.py", line 110, in daemon_process
(IntegratedWorker pid=23605)     os.kill(mgr_pid, signal.SIGKILL)
(IntegratedWorker pid=23605) ProcessLookupError: [Errno 3] No such process
(IntegratedWorker pid=23605) Process ForkServerProcess-10:
(IntegratedWorker pid=23605) Traceback (most recent call last):
(IntegratedWorker pid=23605)   File "/usr/local/python3.10.16/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
(IntegratedWorker pid=23605)     self.run()
(IntegratedWorker pid=23605)   File "/usr/local/python3.10.16/lib/python3.10/multiprocessing/process.py", line 108, in run
(IntegratedWorker pid=23605)     self._target(*self._args, **self._kwargs)
(IntegratedWorker pid=23605)   File "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/utils/common.py", line 110, in daemon_process
(IntegratedWorker pid=23605)     os.kill(mgr_pid, signal.SIGKILL)
(IntegratedWorker pid=23605) ProcessLookupError: [Errno 3] No such process
(IntegratedWorker pid=23750) [ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared! [repeated 120x across cluster]
[ERROR] 2025-07-09-14:16:52 (PID:22866, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
(IntegratedWorker pid=23819)  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1903956992 [repeated 4x across cluster]
